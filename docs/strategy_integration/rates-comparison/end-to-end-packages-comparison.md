End-to-End Plan for 100% Fidelity in WhatsApp-to-Airtable Mortgage Packages
Background & Goals
The goal is to ensure every banker’s WhatsApp message is converted into a structured Airtable “MortgagePackage” entry with zero loss of meaning. The current pipeline processes unstructured WhatsApp updates from multiple banks (DBS, HSBC, OCBC, Maybank, SCB, UOB) and outputs standardized mortgage package data. Early testing indicated strong performance for some banks but gaps for others – DBS/HSBC/OCBC parsing was reliable, while Maybank, SCB, and UOB parsers needed further fixes. A subsequent validation report, however, claims all bank parsers now achieve full parity and Airtable-ready output. This plan reconciles those reports by detailing improvements made and proposing a comprehensive response-aware framework to maintain 100% accuracy going forward.
Key objectives:
•	Accuracy: Achieve and maintain 100% correct data extraction for all banks’ messages (no missed or wrong package details).
•	Consistency: Enforce a unified schema (the Airtable MortgagePackage format) across all outputs.
•	Robustness: Handle various message styles – from pure rate tables to conversational updates – using a meta-framework that coordinates rule-based parsing with AI (LLM) as needed.
•	Guardrails: Prevent semantic drift or data loss through metacognitive tagging, multi-agent hand-offs, and an LCL (locked context ledger) mechanism to preserve context.
•	Scalability: Provide a meta-framework adaptable to new banks, languages, or message formats with minimal changes.
The ultimate deliverable is a prioritized roadmap for system enhancements, an experimental validation strategy for each stage, explicit guardrail checks to ensure quality, and a verification plan aligned with the response-awareness framework guiding the multi-agent system.
Representative WhatsApp Message Examples
To ground the discussion, below are representative WhatsApp message snippets from each partner bank, illustrating the variety of formats the system must handle (rate sheets, promotional updates, policy notices, and hybrid conversational messages):
•	DBS – Rate Sheet (16/07/2025): A structured message with latest rates for various fixed periods, introduced by a brief greeting.

 	Hi! 👋 please see below for updated rates

Completed Property Packages as of 16/07/2025
... (multiple packages with 1YR/2YR/3YR fixed rates and thereafter rates)
•	HSBC – BUC Rates (11/07/2025): A concise structured update focusing on Building Under Construction (BUC) loan rates for large loans.

 	HSBC BUC Rates  
>$1m loan  
Y1-4: 1/3M SORA + 0.30%  
TA: 1/3M SORA + 0.60%  
... (additional tiers and details)
•	OCBC – Technical Rate Sheet (17/07/2025): A detailed rate sheet with internal annotations.

 	STRICTLY NO CIRCULATION  
As of 17/07/[redacted]K  
HDB 3M SORA .5 .5 .75 .75  
1M SORA Completed …  
... (multiple lines of rates for different property types and tenures)
•	Maybank – Hybrid Update (23/06/2025): A hybrid message combining a casual note from the banker with structured rate info.

 	Hi, changes to fixed and SORA packages, i will be back to work on 8 July :) 

23 June 2025  
... (Fixed rates for Year 1-3, SORA floating rates, etc., including notes on cash gifts)
•	SCB – Comprehensive Table (10/07/2025): A complex tabular update (from a PDF) with structured packages split by property segment and client tier.

 	## Residential Properties (Completed) – 10/07/2025

### SORA Benchmark – 1M or 3M Compounded SORA [2 Years Lock-in]  
... (Table rows with loan amounts, preferential banking tiers, Y1–Y4 rates, thereafter rates)
•	UOB – Promotional Rate Sheet (07/07/2025): A structured list highlighting a new 5-year fixed promo package.

 	Latest updated as of 07/07/2025

5 Years Fixed  
Y1-5: 2.45% (HDB >400k) – New  
... (other packages with their rates and conditions)
Each example demonstrates the schema diversity (different tenures, base rates, special features) and sometimes a mix of narrative text plus data. The system’s classifier should detect the message type (e.g. rate sheet vs. conversational update) and route it appropriately: e.g. pure structured tables directly to a rule-based parser, vs. hybrid messages to a combined approach. The above Maybank example is clearly hybrid – a conversational intro (“Hi, ...”) followed by structured rates – requiring the system to capture both the banker’s commentary and the detailed packages.
(For reference, a library of gold-standard outputs for such examples is maintained – e.g. in manual-test-results.json and test-real-banker-examples.ts – to validate parser correctness against expected field values.)
Initial Performance Analysis: Gaps and Conflicting Assessments
Early Testing (18 examples across 6 banks): Initial analysis reports showed high extraction accuracy for DBS, HSBC, and OCBC messages, but flagged serious gaps for Maybank, SCB, and UOB. In an AI Parser test summary, those three banks were marked “⚠️ NEEDS VALIDATION” while the others were “✅ WORKING”【21†】. Specific issues noted included parsing failures on certain bank-specific formats and features. For example, the test plan explicitly called out the need to “address parsing failures for Maybank, SCB, UOB” before deployment. Additionally, classification of message types was not perfect – e.g. some promotional or “quick update” messages were misclassified – indicating room to improve the message classifier logic.
Later Validation (Final Parser Report): After targeted improvements, a comprehensive validation was conducted, and the findings were overwhelmingly positive. The final parser validation report claims 100% accuracy in extracting structured data for all tested messages, with all requirements met for Airtable integration. According to this report, every expected mortgage package was correctly captured for Maybank, SCB, and UOB (closing the earlier gap), and the output data conforms exactly to the Airtable schema. The final checklists show full coverage of bank-specific patterns and features, and that “all extracted packages [are] ready for direct Airtable ingestion” with proper field types and naming conventions. An example of the normalized output format for a Maybank package is given in the report, illustrating the schema alignment:
{
  "bankName": "Maybank",
  "packageName": "Maybank 200k to 399k",
  "year1InterestType": "Fixed",
  "year1Interest": 2.30,
  "thereafterInterestType": "1M SORA + 1.00%",
  "thereafterInterest": 0,
  "lockInPeriod": 2,
  "minLoanAmount": 200000,
  "propertyType": ["HDB", "Private"],
  "remarks": "Free Conversion after 24 months; Cash Gift 0.40% capped at S$2,800"
}
(Fields like bankName, interestType descriptors, numeric rates, arrays for propertyType, etc., match the defined MortgagePackage interface in the codebase. No additional transformation is needed before uploading to Airtable.)
Reconciling the Conflict: The discrepancy between the initial “shaky” status and final “all green” status can be explained by the interim development work: Parsers for Maybank, SCB, UOB were enhanced to handle their unique formats (e.g. Maybank’s inclusion of cash gift incentives, SCB’s multi-tier tables, UOB’s special fixed-rate variants), and the classification logic was refined. Indeed, an internal consolidation effort (“Track A”) achieved ~60% elimination of duplicate logic across bank-specific parsers, suggesting a unification of parsing rules that likely resolved many inconsistencies. By the time of final validation, the system had incorporated these fixes, yielding parity across all six banks.
However, it’s important to verify whether classification accuracy and edge-case handling in production truly meet the 100% structured data extraction claim. The final report notes an 89% correct parser selection (optimal path chosen for 16/18 cases), meaning there is still a small fraction of messages where the initial classification might pick the wrong parsing strategy (e.g. treating a promotional message as a rate sheet or vice versa). While this did not prevent extracting the data eventually (since the structured data was recovered with near-perfect accuracy), it highlights an area for improvement. The plan moving forward will fortify the classification step and implement safety nets so that even if a message is initially misclassified, the system can recover or reroute to ensure no loss of information.
Response-Aware Multi-Agent Framework
To handle the variety of message types and ensure no context is lost, the system employs a response-awareness framework with a multi-agent architecture. The key components and flow are:
•	Message Classification Service: On receiving a message, the first agent (classifier) analyzes it and tags the message into one of several categories:
•	Rate Sheet – highly structured content (tables, lists of rates) → use Rule-Based Parser.
•	Promotional/Free-text Update – primarily narrative or irregular format → use LLM-based Parser.
•	Hybrid (Conversational + Structured) – a mix of commentary and data (e.g. banker’s note followed by a table) → use Hybrid Parser Pipeline (combined approach).
•	Other/Uncertain – if unsure, default to a safe parsing mode (e.g. treat as hybrid or send for manual review).
Classification is currently rule-based (pattern matching on content features), with known limitations (it achieved ~83% on initial tests for some categories). We will improve this by incorporating more robust features (e.g. presence of certain keywords or rate patterns) and even a fallback LLM classifier for borderline cases. Ensuring the correct parser route is chosen is critical, as it triggers the appropriate downstream agent or combination of agents.
•	Rule-Based Parser: A deterministic parser optimized for structured rate sheet content. It uses bank-specific rules/patterns to extract fields (interest rates, tenors, lock-in, etc.). For example, it knows the format of DBS’s fixed deposit rates versus HSBC’s SORA spreads, etc. Over time, much of this logic has been consolidated to reduce duplication – the parser can handle multiple banks by toggling schema configurations or pattern sets. The rule-based parser outputs a list of MortgagePackage objects (one per package identified in the message) in the standard schema. This parser is fast and precise for well-formatted inputs but can miss context or handle only known patterns.
•	LLM-Based AI Parser: This component (powered by GPT/Claude or similar) is invoked for unstructured or complex messages (e.g. a promotional message describing a new product in prose, or a policy update). It uses natural language understanding to interpret the message and extract the relevant mortgage package info. For instance, it might parse a paragraph about a “limited-time offer of 2.3% fixed rate for first 2 years for loans above $500k” into the structured fields. The LLM parser is guided by instructions to output data in the standardized format (to avoid hallucination or format errors). It’s used sparingly (only ~2 of 18 test cases required it), but it’s crucial for those cases where rigid rules fall short. We include guardrails (discussed later) to validate the LLM’s output against expected schema and values (e.g. cross-check that any rate percentage it outputs actually appeared in the text).
•	Hybrid Parser Pipeline: For messages containing both conversational text and structured data, we employ both parsers in sequence, orchestrated by a meta-agent:
•	Context Extraction (Conversational Parser): First, an LLM or specialized agent reads the introductory/free-text portion to extract any contextual info (e.g. “what changed”, banker’s remarks, clarifications about packages). This yields a summary or list of contextual points.
•	Package Extraction (Structured Parser): Next, the rule-based parser (or a structured data extractor) processes the subsequent rate table or list, extracting the packages exactly as it would for a pure rate sheet.
•	Intelligence Fusion (Hybrid Engine): Finally, the system fuses the conversational context with the structured data. This could mean annotating the extracted packages with the contextual insights or including the banker’s comments in a remarks field of the MortgagePackage. For example, if the message says “Not much changes to BUC other than the FHR easy switch packages…” before listing rates, the fusion step will ensure that remark (“FHR easy switch packages changed for BUC”) is attached to the relevant packages in the output. The result is a set of packages that not only have the numeric rates but also the nuanced commentary from the banker, ensuring zero loss of information from the message.
This multi-agent design is managed via metacognitive tags and agent hand-offs. Each stage’s output is tagged (e.g. <CONTEXT_EXTRACTED>..., <PACKAGES_EXTRACTED>...) so that subsequent agents know what data they are receiving. These tags function as a communication protocol in the prompt pipeline, preventing confusion when multiple agents’ outputs are merged. The use of explicit tags and structured intermediate outputs is a lesson learned from internal methodology research – it keeps the system’s “thought process” transparent and helps verification agents double-check each stage’s result. For instance, the conversational parser might output: <CONTEXT_NOTE>Banker mentions X and Y changes</CONTEXT_NOTE>, and the hybrid engine will then incorporate that into package remarks.
Throughout this pipeline, an LCL (Local Context Ledger) mechanism acts as a safeguard against semantic drift. The concept is to maintain a context blueprint of the message’s meaning that persists across agents. Concretely, when the classifier identifies the message type and initial context, a representation of the essential content is stored (the “blueprint”). As each agent works (especially the LLM), it references this blueprint to ensure consistency. This was found to be effective in practice – “LCL context placement” ensured that architectural decisions stayed in context without repeated discussion. In other words, the system doesn’t allow an LLM to wander off-topic or reinterpret the message inconsistently: the LCL holds key facts (dates, rates, names, etc.) locked in, and any output that deviates triggers a drift detection. (Notably, the team observed that without LCL blueprint storage, some misalignment issues would slip past drift detection – underscoring the importance of this safeguard.)
Finally, a Claude/Agent routing protocol is used to orchestrate these steps. Internally, special commands (e.g. /classify, /parse-rule, /parse-llm, /merge) direct the query to the appropriate agent (some agents run on GPT-4, some on Claude, etc., each with tailored instructions). This ensures that, for example, the rule-based parsing agent doesn’t attempt tasks outside its scope, and the LLM agent is constrained to follow the schema format when generating output. The meta-controller oversees the sequence, using the tags and LCL context to verify each step before moving to the next. This response-awareness framework – where the system is aware of its own multi-step reasoning – provides transparency and error-checking at each stage.
Unified Meta-Framework for All Banks
The user has requested a meta-framework that can work for all different banks, encapsulating both rule-based and LLM-driven methods. The proposed solution is to generalize the parsing architecture such that adding a new bank or format doesn’t require a completely new parser – instead, the system can handle it through configuration or minimal extension:
•	Parser Consolidation: Continue the effort from Track A (which already achieved ~60% code duplication reduction) to create a single modular parser for all banks. This would involve abstracting common patterns (e.g. interest rate line formats, table structures) and then using bank-specific modules or config files for the small differences (like specific keywords or special fields that only some banks use). For example, the parser could have a library of rate line regex patterns and choose the appropriate set based on bankName. Many banks share similar formats (e.g. UOB and OCBC both listing rates per property type), so a unified parser with toggles is feasible. This reduces maintenance and ensures improvements benefit all banks uniformly.
•	Bank-Specific Extensions: For truly unique features (e.g. Maybank’s “green loan” flag, or SCB’s private banking tier labels), the framework will allow plug-in components. These could be small functions that post-process the extracted packages to add those extra fields or handle exceptions. The key is the interface remains the same. In code, this might be an interface like BankParserExtension that can be implemented for each bank’s quirks (if needed), while core parsing logic remains centralized.
•	Dynamic Format Recognition: The meta-framework could incorporate a smart format recognizer. Instead of solely relying on the classifier’s coarse categories, the parser itself can detect patterns and adapt. For instance, if a new bank’s message comes in and it has a table with 5 columns labeled in a certain way, the parser can dynamically map those columns to the standard fields (perhaps by using a tiny internal knowledge base of common terms, or even by asking an LLM to map unfamiliar terms to known schema fields as a one-time assist). This flexibility means the system can onboard new banks (e.g. Citibank, Standard Chartered Premier, etc.) quickly. In fact, an internal analysis flagged a coverage gap of 9 banks (Citibank, HSBC Premier, BOC, Maybank Premier, RHB, Hong Leong, CIMB, etc.) that are not yet handled【51†】 – the meta-framework will target these by design, ensuring it’s broad enough to parse their messages once sample data is available.
•	Unified Hybrid Handling: The meta-framework will treat “hybrid” as a first-class message type across banks. Initially, it was observed that when a banker’s message mixed commentary with data, the context could be lost by the purely rule-based part. The new framework will universally apply the 3-step hybrid parsing strategy whenever conversational text is present (regardless of bank). This means even if, say, Citibank messages (not yet in the set) include a preamble from a RM (relationship manager), the system will correctly capture that note. By formalizing the hybrid parser approach (context extraction → data extraction → fusion) as a reusable pipeline, we ensure all banks benefit from this capability. The Phase 2 design already outlines this generalized hybrid process, which we will implement across the board.
•	Normalization & Rate Calculations: Different banks express things differently – e.g. some give explicit “Spread + SORA” formulas, others directly a percentage; some mention lock-in in years, others in months, etc. The meta-parser will include a normalization layer that converts various expressions into the standard form. For example, if a bank says “Lock-in: 24 months”, the parser will output lockInPeriod: 2 (years) for consistency. If a bank provides a “thereafter rate” implicitly by naming a base rate, the parser will compute or fill it in standardized terms (ensuring we always have a numeric thereafterInterest and a descriptive thereafterInterestType). Such normalization rules will be centrally defined but activated per bank as needed. This directly supports the Airtable schema compliance – the final data for any bank will always adhere to the expected types/units.
Crucially, by unifying the framework, maintenance and future updates become easier. If one bank changes format or a new regulation affects all (e.g. introducing a new reference rate), we update one place in the code. The combination of rule-based rigor and LLM flexibility ensures that we’re not relying on a single brittle approach. The LLM component in the loop can help adapt to unforeseen changes in format by interpreting them on the fly, while the rule-based backbone guarantees structured output.
Meta-Framework in Action: In practice, when a new or changed format is encountered, the system can do something like: run the message through the generic parser; if confidence in parsing is low or fields are missing, engage an LLM agent with a prompt like “Here is an unknown format from {bank}. Extract the fields X, Y, Z.” and then integrate that result back. This way, even new patterns can be handled semi-autonomously, and over time the rule config can be updated with the new pattern (learning from the LLM’s interpretation). This approach will be formalized as part of the framework’s continuous learning loop.
Guardrails: Ensuring Quality and Preventing Drift
With multiple agents (and especially with an LLM in the mix), guardrails are essential to ensure the output is always correct and consistent. The plan includes several layers of guardrails and QA checks:
•	Schema Validation: After parsing, every MortgagePackage object is validated against the schema. Required fields must be present and of the correct type (e.g. interest rates as numbers, not strings; arrays like propertyType properly filled). If the LLM parser returns text that doesn’t map cleanly to the schema, that’s flagged as an error. We will implement a strict JSON schema check on the LLM’s output – if it’s not valid, the system can either correct format errors or fall back to a safer parsing method. This guarantees that only Airtable-compatible data (with the exact fields as in the MortgagePackage interface) proceeds.
•	Metacognitive Tags & Checks: As mentioned, intermediate outputs carry tags. A verification agent monitors these. For example, if the conversational parser’s <CONTEXT_NOTE> seems unrelated to the packages extracted (or empty when there was clearly some text), the verifier will raise a flag. Likewise, if the rule-based parser’s <PACKAGES> count is suspiciously low given the message length, it might indicate it missed something. The framework documentation emphasizes using these meta-tags to track the process; indeed, it was identified that some implementation agents in prior runs forgot to use tags, which is to be corrected. We will enforce tagging discipline so that every key piece of info is explicitly marked and can be checked.
•	LCL (Local Context Lock): The LCL safeguard is a pivotal drift-prevention measure. Essentially, once the message is classified and the initial context established, the core facts (customer type, interest figures, etc.) are locked in a shared context. If an LLM tries to introduce something not in that locked context (semantic drift), a discrepancy will be detected. For example, if the message said “2.30%” and the LLM returns “2.50%” in a package, the LCL check will catch that. The system can either automatically correct it (preferably by re-checking the original text) or mark the output for manual review. The blueprint storage means each agent doesn’t start from scratch – it inherits the context state. During testing, it was noted that having this context blueprint was crucial because purely reactive drift detection might not catch subtle misalignments. We will maintain a structured representation of the input (e.g. a dictionary of expected values or a copy of the raw text) accessible to all agents for cross-reference.
•	Consistency & Sanity Checks: Additional automated checks will run on the final packages:
•	Package count sanity: The number of packages extracted is compared against heuristics. For instance, if a rate sheet lists rates for 4 tenors and 3 property types, we expect 12 packages (4×3). The system can infer expected count from patterns (as was done in analysis) and compare with actual extracted count. A mismatch indicates a possible miss or duplicate.
•	Rate format compliance: Ensure interest rates are in a standardized format (e.g. base rate “1M SORA + X%” vs absolute percentages) across all packages. This was effectively 100% in final tests and must remain so.
•	Field interdependencies: If year1InterestType is "Fixed", then year1Interest should be >0 (and no base rate in year1InterestType); if thereafterInterestType contains "SORA", then thereafterInterest might be 0 (since it’s a formula) – these kinds of logical rules ensure the data makes sense. We’ll encode such rules to catch anomalies (e.g. a package with minLoanAmount extremely high or lockInPeriod that doesn’t match what the message implied).
•	Narrative vs Data coherence: For hybrid messages, check that any claim made in the text (like “no changes except X”) is reflected in the data (only X changed). This is harder to automate, but even simple checks like searching the remarks for keywords found in the context note can verify the fusion worked.
•	Agent Hand-off Verification: Each hand-off between agents will have an acknowledgment. For example, after the rule-based parser finishes, a verification step compares the raw text segments to the parsed output to ensure nothing glaring was omitted. If the banker mentioned "cash gift 0.40% capped at $2,800" and that phrase isn’t found in any output field, the system should flag it – meaning the fusion step missed adding it to remarks. By diffing important tokens between input and output, we can catch such omissions.
•	Fallback and Escalation: As a final guardrail, if a message is too novel or the parser confidence is low even after using both approaches, the system can mark the entry as needing human verification before Airtable ingest. The design target is to minimize this, but it’s better to have a controlled fallback than to silently produce wrong data. The confidence scoring will be based on the checks above (schema valid, all checks passed = high confidence; any warnings = low confidence).
These guardrails, combined with the response-aware architecture, ensure that even as the system becomes more autonomous, it does not sacrifice reliability or interpretability. Every step is either automatically verified or easily reviewable, preventing regressions in accuracy.
Prioritized Roadmap
To implement the above, we outline a step-by-step roadmap with priorities:
1.	Finalize Parser Consolidation (High Priority): Complete the unification of rule-based parsing logic across all existing banks. Remove any remaining redundant code and encapsulate bank-specific patterns in configuration files or modules. This includes unifying terminology (e.g. ensure one consistent way to detect “floating” vs “fixed” across banks) and integrating the hybrid parsing hooks into this unified parser. Deliverable: A single RuleBasedParser module covering all 6 banks with toggled settings, passing all current tests.
2.	Boost Classification Accuracy (High Priority): Improve the MessageClassificationService to reduce misroutes. This involves updating pattern rules (incorporate new keywords from the latest messages), using small ML or heuristic models to distinguish message types, and possibly an ensemble approach where an LLM double-checks the classification for ambiguous cases. We will specifically target the promotional vs rate-sheet confusion that led to only ~11% accuracy in that category initially. The goal is to exceed 95% classification accuracy on known examples and establish a confidence measure – if the classifier is below a confidence threshold on a message, mark it as “hybrid” or have a secondary agent choose the parser path to be safe. Deliverable: Updated classifier with >95% accuracy on the test suite and logic to handle low-confidence cases (either via fallback to a safe parse or asking a clarifying question if interactive).
3.	Implement Hybrid Parser Phase 2 (High Priority): Deploy the 3-step Hybrid Parsing process for any message that contains both free text and structured data. This involves creating the conversational context extractor (likely an LLM prompt template that extracts bullet-point notes from the intro text) and the fusion mechanism to merge notes into package remarks or annotations. We’ll test this on known hybrid examples (like the Maybank message) to ensure the commentary is preserved in output. Deliverable: A HybridParser component that, given a message, orchestrates the context extraction and data extraction and returns enriched MortgagePackages. This should be integrated such that the classifier can invoke HybridParser when needed.
4.	LCL Context Blueprint Integration (Medium Priority): Develop the infrastructure for LCL. Concretely, define a data structure to hold key message context (e.g. a mapping of package identifiers to expected values, or a summary of all content) which is passed along to each agent. Implement drift detection checks that compare agent outputs to the LCL blueprint. This might also involve instrumenting the LLM prompts to include the blueprint (e.g. “Here are facts you must not contradict…”). Initially, test this on a few scenarios where drift is a risk (maybe intentionally perturb an LLM output to see if the system catches it). Deliverable: An LCL module that snapshots context at classification stage and APIs for agents to retrieve it and for a verifier to validate outputs against it.
5.	Extended Bank Coverage (Medium Priority): Begin incorporating additional banks or variations. Using the meta-framework, attempt to parse a few examples from banks outside the original six (if available). This will validate how easily the system can adapt. We know from design docs that about 9 banks are pending coverage【51†】 – plan a strategy to incrementally add those. Likely, prioritize one or two (e.g. Citibank and RHB) and get their message format, then add their patterns to the unified parser and test. This step ensures our framework is indeed generalizable. Deliverable: Support for at least 2 new banks’ message examples, demonstrated by successful parsing into the schema (even if partial features).
6.	User Acceptance Testing & Schema Lock (High Priority): Before fully automating Airtable imports, conduct a thorough UAT with real or simulated messages to ensure the output is perfect. This includes verifying that the Airtable field locking and formatting requirements are all satisfied – e.g., some fields might be read-only or formula-driven in Airtable, so confirm we only populate what is allowed. Also verify any downstream automations (for instance, if Airtable triggers notifications on new records) are not disrupted by our insertion. This step essentially “freezes” the output schema: no field mismatches or naming errors can be tolerated. Deliverable: A sign-off that for each field in MortgagePackage, our output is correctly formatted (e.g. interest rates to 2 decimal places, strings not exceeding length limits, etc.), as evidenced by a final test ingestion into Airtable with zero errors.
7.	Experimental Scaling & Edge Cases (Low Priority / Ongoing): Design experiments to stress-test the system. For example, generate synthetic messages that combine elements from multiple banks (to simulate a completely novel format) and see if the parser can still handle it. Or have an LLM “mutate” known messages (insert typos, reorder sections) to test robustness of the rule-based parser. These experiments will reveal any brittle points in the logic. Particularly, test the semantic drift scenario: feed the LLM parser ambiguous inputs and see if it stays faithful with LCL in place. Also simulate high volume to ensure the multi-agent orchestration doesn’t time out or conflict (the parallel agent execution noted in internal docs suggests the system can run multiple agents simultaneously, which we should leverage for efficiency). Deliverable: An internal report on these experiments, with any necessary adjustments to the parser or classifier based on the findings (e.g. if a certain bank’s message consistently confuses the classifier, add new rules; if a new slang/abbreviation appears in WhatsApp texts, teach the parser to understand it).
8.	Monitoring & Continuous Learning Setup (Medium Priority): Establish a monitoring pipeline for when this system runs in production (ingesting live WhatsApp updates). Key metrics to track: parsing success rate (did it produce a package or fall back to manual?), number of packages extracted per message vs. expected, classification distribution (to catch drift in the proportion of message types – e.g. if suddenly many messages are “unclassified”), and accuracy audits where we periodically manually verify a sample of outputs. Set up alerts for anomalies, such as a spike in parser errors or significantly low package counts (which might indicate a format change by a bank). Additionally, maintain a feedback loop: when bankers’ messages evolve (new phrasing, new products), quickly incorporate those into the rules or training data. Deliverable: A dashboard or log system that captures each message processed, what path was taken, and any flags raised by guardrails, enabling the team to respond in near real-time to any issues.
The roadmap is prioritized to address the most critical needs first (ensuring the core system is reliable for existing banks with current formats), then extending capabilities (hybrid, new banks, drift prevention), and finally establishing a robust framework for long-term maintenance and monitoring.
Experimental Design for Validation
At each stage of the roadmap, we will employ careful testing to validate the approach:
•	Unit Tests for Parsing Rules: Expand the unit test suite for the rule-based parser covering every bank’s known message patterns. For example, ensure the parser correctly handles a Maybank rate line with a “green” loan tag or an OCBC line with an odd spacing. Use the __fixtures__/ message texts as test inputs and assert that the output JSON matches the expected gold-standard (from manual-test-results). This was partially done before; we’ll update expected outputs for any new fields added (e.g. if we now capture a previously ignored field like a special remark, the test expectations should include it).
•	Simulation of Real-world Runs: Using the 18 canonical WhatsApp examples, perform full pipeline runs in a sandbox environment for each after each major code change. The sandbox will mimic the actual multi-agent orchestration (perhaps with some agents stubbed or using recorded outputs to isolate components). This end-to-end test is crucial whenever the classifier or parser logic changes, to ensure nothing breaks integration. Logs from these runs (similar to baseline-run.log and validation-output.log) will be reviewed to confirm that no new errors are introduced and that all guardrail checks pass (in the final validation-output.log from last run, there were no error flags【57†】, which is the standard we maintain).
•	A/B Testing Parser Strategies: For hybrid messages and borderline cases, we will conduct A/B experiments to determine the best approach. For example, we might compare:
•	Approach A: Treat a borderline message as structured (rule-based only) vs.
•	Approach B: Treat it as hybrid (LLM to assist) and see which yields more accurate results. This can be measured by correctness of extracted fields against the known truth. This experimental mindset helps ensure we only deploy the more accurate strategy for each scenario. If Approach B consistently adds value (e.g. capturing context that Approach A misses) without errors, then hybrid becomes default for that scenario.
•	Continuous Fuzz Testing: Using the synthetic message generation mentioned, we will fuzz test the system. This means feeding slight random variations of known inputs to ensure the parser isn’t overfit to exact phrasings. The experimental design here is to automate generation of 100s of variations (e.g. shuffle order of packages, insert random greetings or irrelevant lines) and verify the parser still extracts the core packages correctly and ignores irrelevant text. Any failure will highlight an opportunity to make the parser more flexible or improve the classifier’s noise tolerance.
•	User Beta Testing: Before fully rolling out, engage a small group of ops/business users to review the Airtable entries generated from a batch of real messages. Their feedback (e.g. “this field looks off” or “we expected this promo to be tagged differently”) is invaluable. It’s effectively an experimental UAT phase. We’ll incorporate any feedback (for instance, maybe adding a new field if business needs it, or adjusting naming conventions) into the final iteration.
Through these experimental validations, we aim to not just trust the final “100% accuracy” claim but continually prove it under varied conditions. The plan is to have zero regressions: if we fix something for one bank, our test suite and experimental runs should catch if it inadvertently affected another.
Verification & Ongoing Monitoring
Once the enhanced system is deployed, maintaining that 100% fidelity requires ongoing verification and monitoring:
•	Automated Verification Agent: The framework already includes verification agents (one per implementation agent). We will extend their role in production. After each message is processed and a MortgagePackage record is created, a verification agent will run a final check (as described in Guardrails) on the output. This agent’s report (even if all checks pass) will be logged. If any check fails, it can automatically revert the entry (or mark it as needing correction) and notify the team. Having this real-time verification is like a continuous audit.
•	Dashboard & Alerts: Set up a live dashboard that tracks key metrics: number of messages processed, packages created, any errors or warnings. Particularly, track classification accuracy over time – if we notice an uptick in messages going down the “LLM-required” path that were actually simple rate sheets, that indicates classifier drift or new patterns we didn’t account for. We will also monitor the frequency of guardrail triggers. For example, if the LCL drift check flags messages occasionally, we inspect those cases to understand why (was it a new phrasing that confused the LLM?). The monitoring system will have alert thresholds – e.g., “Less than 98% of messages parsed without warnings in the last week” triggers an email/slack alert to engineers.
•	Periodic Retraining/Updating: Although much of the system is rule-based, the classifier or LLM prompt might need updates as new data comes in. We plan a quarterly review of random samples of incoming WhatsApp messages against the Airtable outputs. Any error (even minor) will be catalogued and used to update rules or training data. If a pattern of new language appears (say a bank starts using a new acronym or format), we will proactively update the parser. This ensures the system stays current with evolving business usage.
•	Confidence Metrics & SLA: We will define an internal SLA (Service Level Agreement) for the system’s performance – e.g. 100% of messages should be parsed with no critical errors. “Critical” meaning a wrong interest rate or package name that could mislead decisions. We aim for 0 such critical errors. Non-critical issues (like a minor formatting issue in remarks) should be under 5% of messages. These metrics will be tracked. Additionally, a “confidence score” can be logged for each message (based on classifier confidence, guardrail passes, etc.). Over time, if we see confidence dipping for a certain bank’s messages, that’s a signal to investigate before any error occurs. The team can use these metrics to decide if/when to retrain an LLM component or refine a rule.
•	Drift Monitoring: Specifically for semantic drift, beyond LCL’s immediate checks, we will analyze trends. For instance, if an LLM’s outputs start deviating subtly (could happen if the model is updated or if the style of messages changes), we want to catch that early. We can maintain a small set of canary messages – fixed inputs that we run through the system periodically – to see if the outputs change over time. If a difference is detected (and not justified by input changes), it means drift either in our model or in how it’s interpreting the prompt, prompting a review.
•	Logging and Traceability: Every decision in the pipeline is logged with the meta tags, so we can trace exactly how a particular field was derived (this is part of the response-aware philosophy). If a stakeholder questions a data point in Airtable, we should be able to go back to the logs and see, for example, “Interest rate 2.30% was extracted by RuleBasedParser from text ‘2.30% p.a.’ on line 5 of the message.” This traceability builds trust and makes debugging easier if any issue arises.
In summary, verification is not a one-time task but a continuous process. By combining automated checks, live monitoring, and periodic manual reviews, we will keep the system at the 100% accuracy level over time. The response-awareness framework’s transparency means we can quickly pinpoint which stage might be responsible if an anomaly occurs (e.g., classification vs parsing). And the meta-framework’s adaptability means when new challenges appear (new banks or formats), we can respond within the same architecture.
Conclusion
Through a combination of unified parsing logic, multi-agent collaboration, and robust guardrails, this plan ensures that every WhatsApp message from partner banks can be transformed into an Airtable-ready mortgage package entry with no loss of information or accuracy. We have reconciled prior inconsistencies by strengthening the weak links (Maybank/SCB/UOB parsing and message classification), and we’ve put in place a framework that is aware of its own processes – using meta-cognitive tags and LCL context to avoid mistakes even as complexity grows.
The deliverables include a prioritized implementation roadmap (focused first on consolidation, classification, and hybrid parsing), an experimental validation regimen to prove each enhancement, and a monitoring plan to uphold quality continuously. By adhering to the Airtable schema contract and leveraging the response-awareness methodology, the system will produce outputs that are immediately usable by downstream workflows (no manual cleanup needed).
Importantly, this approach is scalable: as new banks or message styles come on board, the meta-framework can accommodate them with minimal friction, moving us closer to a universal solution for structured data extraction from unstructured communications. The conflicting test results have been resolved in favor of a system that meets the highest accuracy standards, and with the outlined strategy, we will keep it that way – every mortgage package, every message, every time, parsed correctly.
________________________________________
