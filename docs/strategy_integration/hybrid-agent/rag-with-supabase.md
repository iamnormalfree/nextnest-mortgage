Yes â€” using Supabase for RAG is actually a great choice in your stack. Itâ€™s production-grade, scales easily, and plugs in cleanly with Next.js, Vercel AI SDK, and BullMQ.

Hereâ€™s the trade-off and implementation view so you can decide confidently ğŸ‘‡

ğŸ§­ TL;DR Recommendation

âœ… Use Supabase (pgvector) for RAG if you:

Already use Supabase for auth or user data.

Want cloud persistence and easy SQL inspection.

Need multi-tenant data separation (e.g., per client/project KB).

Plan to scale past a few thousand docs or need filters (tags, doc type, recency).

ğŸš« Stay with sqlite-vss / local embeddings if you:

Are still prototyping or have <500 docs.

Need zero latency (running on Vercel Edge Functions).

Donâ€™t want to manage a cloud DB connection in serverless.

âš™ï¸ How RAG looks with Supabase
1. Set up pgvector

Supabase Postgres comes with pgvector pre-installed.

create extension if not exists vector;
create table knowledge_chunks (
  id uuid primary key default gen_random_uuid(),
  doc_id text,
  content text,
  embedding vector(1536),
  tags text[],
  created_at timestamptz default now()
);
create index on knowledge_chunks using ivfflat (embedding vector_cosine_ops);

2. Generate embeddings

Use your model of choice (OpenAI text-embedding-3-small is fine for now):

// scripts/embed.ts
import { createClient } from '@supabase/supabase-js'
import OpenAI from 'openai'

const supabase = createClient(process.env.SUPABASE_URL!, process.env.SUPABASE_KEY!)
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! })

async function embedDoc(docId:string, text:string) {
  const { data: emb } = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: text
  })
  const vector = emb.data[0].embedding
  await supabase.from("knowledge_chunks").insert({
    doc_id: docId, content: text, embedding: vector
  })
}

3. Retrieval query

At inference time:

import { createClient } from '@supabase/supabase-js'

export async function retrieveRelevant(query:string, k=4){
  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! })
  const supabase = createClient(process.env.SUPABASE_URL!, process.env.SUPABASE_KEY!)
  const { data: emb } = await openai.embeddings.create({
    model:"text-embedding-3-small",
    input:query
  })
  const embedding = emb.data[0].embedding
  const { data } = await supabase.rpc("match_knowledge_chunks", {
    query_embedding: embedding,
    match_threshold: 0.75,
    match_count: k
  })
  return data // [{id, content, similarity}]
}


And define this Postgres function once:

create or replace function match_knowledge_chunks(
  query_embedding vector(1536),
  match_threshold float,
  match_count int
)
returns table(id uuid, content text, similarity float)
language plpgsql as $$
begin
  return query
  select kc.id, kc.content,
         1 - (kc.embedding <=> query_embedding) as similarity
  from knowledge_chunks kc
  where 1 - (kc.embedding <=> query_embedding) > match_threshold
  order by similarity desc
  limit match_count;
end;
$$;

ğŸ§© Integrating into your Next.js agent

Your lib/rag/retriever.ts now becomes:

import { retrieveRelevant } from "@/lib/supabase/rag"

export async function search(query:string,k=4){
  const matches = await retrieveRelevant(query,k)
  return matches.map(m => ({
    id: m.id,
    text: m.content,
    score: m.similarity
  }))
}


Thatâ€™s it â€” your Next.js API routes and BullMQ workers continue unchanged.

ğŸ’¡ Best practices for Singapore mortgage RAG
Data Type	Refresh	Notes
MAS circulars / TDSR/MSR rules	yearly	Store entire notice text + summary embeddings
Bank product sheets	monthly	Use version tag (e.g. DBS_2025_10)
Macro briefs (Fed, SORA)	weekly	Tag by date, embed short summaries
Persuasion templates (neutral messaging)	static	Keep under /data/kb but also embed to allow tone grounding

You can include a tags filter in retrieval ({ tags: ['macro','rates'] }) to bias results.

ğŸš€ Example deployment flow (Vercel + Supabase)

Store docs in Supabase knowledge_chunks.

Edge function (app/api/chat/route.ts) â†’ calls retrieveRelevant(query) via Supabase RPC.

Combine retrieved text â†’ feed into your Claude/Codex model or your hybrid agent.

Optionally cache retrievals in Upstash Redis to cut down Supabase reads.

âš–ï¸ Summary
Feature	sqlite-vss	Supabase (pgvector)
Deployment	local / edge	managed cloud
Scaling	limited	excellent
Query speed	~1â€“2ms local	~10â€“30ms RPC
Multi-tenant	manual	built-in (row-level security)
Observability	none	SQL + dashboard
Recommended for you	âŒ prototype only	âœ… production mortgage RAG

If you like, I can generate the ready-to-deploy Supabase RAG module (SQL + TypeScript client + Next.js integration file) to drop into your existing repo â€” would you like me to scaffold that next?